"use strict";(self.webpackChunkwhg_training_resources=self.webpackChunkwhg_training_resources||[]).push([[1196],{3905:(e,t,n)=>{n.d(t,{Zo:()=>m,kt:()=>h});var a=n(7294);function o(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function r(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){o(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,a,o=function(e,t){if(null==e)return{};var n,a,o={},i=Object.keys(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||(o[n]=e[n]);return o}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var l=a.createContext({}),p=function(e){var t=a.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):r(r({},t),e)),n},m=function(e){var t=p(e.components);return a.createElement(l.Provider,{value:t},e.children)},c="mdxType",u={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},d=a.forwardRef((function(e,t){var n=e.components,o=e.mdxType,i=e.originalType,l=e.parentName,m=s(e,["components","mdxType","originalType","parentName"]),c=p(n),d=o,h=c["".concat(l,".").concat(d)]||c[d]||u[d]||i;return n?a.createElement(h,r(r({ref:t},m),{},{components:n})):a.createElement(h,r({ref:t},m))}));function h(e,t){var n=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var i=n.length,r=new Array(i);r[0]=d;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s[c]="string"==typeof e?e:o,r[1]=s;for(var p=2;p<i;p++)r[p]=n[p];return a.createElement.apply(null,r)}return a.createElement.apply(null,n)}d.displayName="MDXCreateElement"},8760:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>r,default:()=>u,frontMatter:()=>i,metadata:()=>s,toc:()=>p});var a=n(7462),o=(n(7294),n(3905));const i={sidebar_position:1},r="Warmup: examining read coverage in a region",s={unversionedId:"statistical_modelling/Hidden_markov_models/glycophorin_cnv_warmup",id:"statistical_modelling/Hidden_markov_models/glycophorin_cnv_warmup",title:"Warmup: examining read coverage in a region",description:"Welcome! The practical studies a region of the human genome on chromosome 4 that is known to",source:"@site/docs/statistical_modelling/Hidden_markov_models/glycophorin_cnv_warmup.md",sourceDirName:"statistical_modelling/Hidden_markov_models",slug:"/statistical_modelling/Hidden_markov_models/glycophorin_cnv_warmup",permalink:"/whg-training-resources/statistical_modelling/Hidden_markov_models/glycophorin_cnv_warmup",draft:!1,editUrl:"https://github.com/whg-training/whg-training-resources/edit/main/docs/statistical_modelling/Hidden_markov_models/glycophorin_cnv_warmup.md",tags:[],version:"current",sidebarPosition:1,frontMatter:{sidebar_position:1},sidebar:"sidebar6",previous:{title:"Glycophorin CNV calling tutorial",permalink:"/whg-training-resources/statistical_modelling/Hidden_markov_models/"},next:{title:"Modelling CNVs using a Hidden Markov Model",permalink:"/whg-training-resources/statistical_modelling/Hidden_markov_models/glycophorin_cnv_hmm"}},l={},p=[{value:"First look at the data",id:"first-look-at-the-data",level:2},{value:"Using an empirical model to handle variation in the data",id:"using-an-empirical-model-to-handle-variation-in-the-data",level:2},{value:"Modelling copy number variation",id:"modelling-copy-number-variation",level:2},{value:"Copy number inference using MLEs",id:"copy-number-inference-using-mles",level:2},{value:"Bayesian copy number inference",id:"bayesian-copy-number-inference",level:2},{value:"Future directions",id:"future-directions",level:2}],m={toc:p},c="wrapper";function u(e){let{components:t,...n}=e;return(0,o.kt)(c,(0,a.Z)({},m,n,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"warmup-examining-read-coverage-in-a-region"},"Warmup: examining read coverage in a region"),(0,o.kt)("p",null,"Welcome! The practical studies a region of the human genome on chromosome 4 that is known to\ncontain large ",(0,o.kt)("em",{parentName:"p"},"copy number variants"),". These are genetic variants that occur due to mistakes in DNA\nreplication and lead to deletions, duplications, or other rearrangements of large tracts of DNA. In\ngeneral copy number variants (and other structural variants that don't change copy number, such as\ninversions) are thought to be extremely important determinants of human disease - the CNVs in\nthis practical delete or duplicate whole genes, and some of them are\n",(0,o.kt)("a",{parentName:"p",href:"https://dx.doi.org/10.1126/science.aam6393"},"associatied with malaria susceptibility"),"."),(0,o.kt)("p",null,"On the other hand CNVs and other structural variation are not that well studied, particularly when they\noccur in regions of genome duplication or paralogy."),(0,o.kt)("p",null,"In this practical the plan is to try to genotype CNVs in one such region - the region containing\n",(0,o.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Glycophorin_A"},(0,o.kt)("em",{parentName:"a"},"GYPA")),". ",(0,o.kt)("em",{parentName:"p"},"GYPA")," encodes Glycophorin A, which is one\nof the most abundant red cell surface proteins. Malaria parasites are known to interact with\nGlycophorin A during invasion, and we got interested in this because it turns out that these CNVs\ncan provide protection against malaria. The practical is based on data from this paper\n",(0,o.kt)("a",{parentName:"p",href:"https://dx.doi.org/10.1126/science.aam6393"},"https://dx.doi.org/10.1126/science.aam6393")," where we investigated that protection."),(0,o.kt)("p",null,"To call CNVs we will look at ",(0,o.kt)("em",{parentName:"p"},"sequence coverage data")," (i.e. how many reads aligned to each genomic\nlocation from short-read sequence data) and look for variation in coverage that might indicate loss\nor gain of DNA copies. To make this simple, we have grouped the genome into consecutve 1600bp bins\nand we work with ",(0,o.kt)("em",{parentName:"p"},"mean coverage in each bin"),"."),(0,o.kt)("h2",{id:"first-look-at-the-data"},"First look at the data"),(0,o.kt)("p",null,"First of all let's load and look at the data - we'll use R for this practical (but if you are expert you are welcome to\nexplore other methods)."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-R"},'data = read.delim( "glycophorin_binned_coverage.tsv.gz", header = T, as.is = T, sep = "\\t" )\nView(data)\n')),(0,o.kt)("p",null,"The data has position information in the first few columns, and samples in columns 4 onwards.\nLet's split these things out for easier handling.  We'll call the actual data ",(0,o.kt)("inlineCode",{parentName:"p"},"X"),":"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-R"},"sites = data[,1:3]\nX = as.matrix( data[,4:ncol(data)] )\nrownames(X) = sites$position\nsamples = colnames( X )\n")),(0,o.kt)("p",null,"How could we plot this data?  One way is just to make a heatmap:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"image( X, x = 1:nrow(sites), y = 1:length(samples) )\n")),(0,o.kt)("p",null,"This is not all that edifying but a few features are evident. Clearly samples vary in coverage (some rows are more red\nthan others). Also, some sites seem to have more coverage than others (some columns are more red). Also some sites have\nmissing data! (White columns). The reason for this is that the region contains paralogous gene copies which make read\nmapping difficult - we have excluded bins where mappability was poor."),(0,o.kt)("p",null,"If you stare hard between bins 300-400, you may start to see some samples seem to have something going on (long\nhorizontal bands of more yellow or more red). This is the signal we want to extract.  There are a few ways we could try this - for example, a dimension reduction method, like PCA, might work.  Feel free to try that!  Here we are going to explore a modelling approach."),(0,o.kt)("h2",{id:"using-an-empirical-model-to-handle-variation-in-the-data"},"Using an empirical model to handle variation in the data"),(0,o.kt)("p",null,"Let's look at how coverage actually looks across sites, for the first few samples:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-R"},"par( mar = c( 2, 2, 2, 1 ) )                                    # this line adjust margins to get more on plot\nlayout( matrix( 1:25, byrow = T, ncol = 5 ))                    # put multiple panes on one plot\nfor( i in 1:25 ) {\n    h = hist(\n        X[,i],                      # coverage values for individual i\n        breaks = 25,                # Number of bars to plot\n        freq = FALSE,               # This scales the y axis so density sums to 1, i.e. empirical distribution\n        main = samples[i]           # This is the plot title\n    )\n    grid()\n}\n")),(0,o.kt)("p",null,"The data for each sample looks kind of uni-modal and sort of symmetrical-ish in general, with a few bumps. Of course\nany CNVs might affect that, and that could explain some of the bumps - that's what we want to find out."),(0,o.kt)("p",null,"For a practical approach we will assume that this binned coverage follows a Gaussian distribution. And for a first\nguess, we will fit the gaussian using all the data in the 1st 100 bins (which from the plot above don't obviously seem\nto contain many CNVs). So let's go ahead and compute the parameters of these gaussians now:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"coverage.parameters = data.frame(\n    sample = samples,\n    mean = sapply( 1:ncol( X ), function(i) { mean( X[1:100,i], na.rm = T ) }),\n    variance = sapply( 1:ncol( X ), function(i) { var( X[1:100,i], na.rm = T ) })\n)\nView( coverage.parameters )\n")),(0,o.kt)("p",null,"We're going to use a Gaussian likelihood function to model sequence coverage data.  The ",(0,o.kt)("inlineCode",{parentName:"p"},"dnorm()")," function in R can be used to implement this. In the spirit of making code readable, let's use this to write a 'gaussian.ll()` function that we will use in our code:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-R"},"gaussian.ll <- function(\n    data,\n    params = list(\n        mean = 0,\n        variance = 1\n    )\n) {\n    return(\n        dnorm( data, mean = params$mean, sd = sqrt( params$variance ), log = TRUE )\n    )\n}\n")),(0,o.kt)("p",null,"Let's plot data again and see how the fit looks:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"par( mar = c( 2, 2, 2, 1 ) )                                    # this line adjust margins to get more on plot\nlayout( matrix( 1:25, byrow = T, ncol = 5 ))\nx = seq( from = 0, to = 20, by = 0.01 )\nfor( i in 1:25 ) {\n    h = hist(\n        X[,i],                      # coverage values for individual i\n        breaks = 25,                # Number of bars to plot\n        freq = FALSE,               # This scales the y axis so density sums to 1, i.e. empirical distribution\n        main = samples[i]           # This is the plot title\n    )\n    points(\n        x,\n        exp(gaussian.ll(\n            data = x,\n            params = list(\n                mean = coverage.parameters$mean[i],\n                variance = coverage.parameters$variance[i]\n            )\n        )),\n        type = 'l',\n        col = 'red'\n    )\n    grid()\n}\n")),(0,o.kt)("p",null,"The fit looks ... sort of ok.  Not perfect, but not bad, depending on the sample."),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Exercise")," feel free to see how this looks for other samples.  You could also make a qq-plot for each sample to see how well the model fits."),(0,o.kt)("p",null,"Clearly this model is not a perfect model of the data, but it might be enough to work with - for the purposes of this\npractical we'll go with it."),(0,o.kt)("h2",{id:"modelling-copy-number-variation"},"Modelling copy number variation"),(0,o.kt)("p",null,"Our general model of the effect of CNVs on coverage is that coverage of a site with copy number ",(0,o.kt)("inlineCode",{parentName:"p"},"c")," should be ",(0,o.kt)("inlineCode",{parentName:"p"},"c")," times\nas large as at a site with copy number 1 - plus noise.  (Humans are diploid so the copy number at most sites is 2). If we think of sequence reads as being generated from each\ncopy independently, a bit of thought shows that the same relationship happens for the variance as well. So our model\nfor copy number c would be that"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"     coverage ~ N( c * mean, c * variance )\n")),(0,o.kt)("p",null,"This is good because it provides a direct link from copy number to coverage (accounting for noise in the coverage\nvalues), that might allow us to infer copy numbers."),(0,o.kt)("p",null,"Let's try that now. For simplicity:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"We will assume only copy numbers of 0 to 5 are possible (i.e. from a homozygous deletion up to a possible three extra copies).  Feel free to increase this if you want to!"),(0,o.kt)("li",{parentName:"ul"},"We'll put the results in a giant multidimensional array of ",(0,o.kt)("inlineCode",{parentName:"li"},"sites")," x ",(0,o.kt)("inlineCode",{parentName:"li"},"samples")," x ",(0,o.kt)("inlineCode",{parentName:"li"},"copy number states"),".")),(0,o.kt)("p",null,"We start by computing the per-site likelihood of each data point for each possible copy number. Importantly as in the\nstats modelling session ",(0,o.kt)("em",{parentName:"p"},"we will work throughout in log space")," to avoid any numerical issues. This makes some\ncomputations more difficult, but is a generally good idea when more than a few data points are involved.  So we'll make our array ",(0,o.kt)("em",{parentName:"p"},"an array of log-likelihoods"),":"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},'copy.numbers = 0:5 \ncopy.number.lls = array(\n    NA,                                 # fill with missing data to start\n    dim = c(                            # dimensions\n        nrow(sites),\n        length(samples),\n        length( copy.numbers )\n    ),\n    dimnames = list(                    # A good feature of R is you can name everything\n        sites$position,\n        samples,\n        sprintf( "cf=%d", copy.numbers )\n    )\n)\n')),(0,o.kt)("p",null,"Let's compute these lls:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"for( sample in 1:length(samples) ) {\n    for( i in 1:length(copy.numbers) ) {\n        copy.number = copy.numbers[i]\n        mean.multiplier = copy.number/2\n        variance.multiplier = max( copy.number/2, 0.01 ) # this to be error tolerant, as explained below\n        copy.number.lls[,sample,i] = gaussian.ll(\n            X[,sample],\n            params = list(\n                mean = coverage.parameters$mean[sample] * mean.multiplier,\n                variance = coverage.parameters$variance[sample] * variance.multiplier\n            )\n        )\n        # Handle the missing data sites.\n        # We will just treat missing data as having likelihood 1 (loglikelihood zero) here.\n        copy.number.lls[ is.na(X[,sample]),sample, ] = 0\n    }\n}\n")),(0,o.kt)("p",null,"Look at the top left of the output:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"View( copy.number.lls[1:10,1:10,] )\n")),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Note"),' In practice even sites with 0 "real" coverage might get some coverage, e.g. due to spurious read alignment or mis-alignment.  For that reason we used a ',(0,o.kt)("inlineCode",{parentName:"p"},"variance.multiplier")," variable above.  It equals ",(0,o.kt)("inlineCode",{parentName:"p"},"copy.number / 2")," except for copy number zero, where it allows a small amount of coverage to exist."),(0,o.kt)("p",null,"These copy number log-likelihoods should now capture some of the important signal in the data.  But how to plot it given it's 3-dimensional?  Here are two ways we could do it."),(0,o.kt)("p",null,"We could take the maximum likelihood copy number call and plot that (for each sample and each site)."),(0,o.kt)("p",null,"Or, we could recognise that the above is daft because it doesn't take into account what we know - that most people will be diploid at most sites.  Instead, we could use a Bayesian approach to build this information in."),(0,o.kt)("p",null,"The rest of this practical does both these things.  We'll use this function to plot the results:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-R"},'plot.copy.numbers <- function(\n    copy.number,\n    filename = NULL,\n    title = NULL,\n    palette = c( "darkorange2", "darkgoldenrod1", "forestgreen", "deepskyblue2", "deepskyblue4", "blueviolet" )\n) { \n    if( !is.null( filename )) {\n        pdf( file = filename, width = 6, height = 8 )\n    } \n    par(mfrow=c(1,1))\n    par( mar = c( 4, 4, 2 + 2 *!is.null(title), 2 ) + 0.1 )\n    image( copy.number, x = 1:nrow(copy.number), y = 1:ncol(copy.number), xlab = "sites", ylab = "samples", col = palette, zlim = c( 0, length(palette)-1 ), main = title )\n    legend( "topleft", pch = 22, col = \'black\', pt.bg = palette, legend = c( "hom deletion", "het deletion", "normal", "1 extra copy", "2 extra copies", "3 extra copies" ), bg = "white" )\n    if( !is.null( filename )) {\n        dev.off()\n    }\n}\n')),(0,o.kt)("h2",{id:"copy-number-inference-using-mles"},"Copy number inference using MLEs"),(0,o.kt)("p",null,"Let's compute the maximum likelihood copy number state for each individual at each site:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},'maximum.likelihood.state = array( NA, dim = c( nrow(X), ncol( X ) ), dimnames = list( sites$position, samples ))\nfor( variant in 1:nrow( X )) {\n    for( sample in 1:ncol(X)) {\n        w = which.max( copy.number.lls[variant,sample,] )\n        stopifnot( !is.na(w) )\n        maximum.likelihood.state[variant,sample] = copy.numbers[w]\n    }\n}\n\nplot.copy.numbers( maximum.likelihood.state, title = "Maximum likelihood copy number" )\n')),(0,o.kt)("p",null,"That plot definitely looks cleaner!  Still lots of noise though, can we do better?"),(0,o.kt)("h2",{id:"bayesian-copy-number-inference"},"Bayesian copy number inference"),(0,o.kt)("p",null,"Taking the maximum likelihood approach is daft because we have salient prior information: most people will be diploid\nat most sites, with possibly a few CNVs sprinkled in. Let's build that knowledge in by applying our bayesian reasoning.\nStill working seperately for each sample and each site, we will take a prior distribution that puts most weight on\ndiploid state, and estiamte copy number state by taking posterior expectations."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-R"},"# our prior - note this should sum to one\nprior = c(\n    `cn=0` = 0.02,\n    `cn=1` = 0.02,\n    `cn=2` = 0.9,       # I put 90% weight on diploid state...\n    `cn=3` = 0.02,      # ...and 2% weight on everything else - feel free to try different values (make it sum to 1)\n    `cn=4` = 0.02, \n    `cn=5` = 0.02\n) \n")),(0,o.kt)("p",null,"Here is another big array to put our results in:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-R"},"expected.posterior.state = array(\n    NA,\n    dim = c( nrow(sites), length(samples) ),\n    dimnames = list(\n        sites$position,\n        samples\n    )\n)\n")),(0,o.kt)("p",null,"Now let's compute it."),(0,o.kt)("p",null,"Here a small potential technical problem occurs. To compute the normalisation factor in Bayes rule we need to sum over\nthe possible copy numbers. (Just like with the fair/unfair dice example fom the stats modelling session.) The\nprobabilities for different copy numbers vary widely in magnitude (from close to -Inf to > 1) and this is a classic\ncase that causes numerical difficulties. To solve that we are working in log space."),(0,o.kt)("p",null,"But to compute expectations we now need to compute a sum over probabilities. A direct approach would use ",(0,o.kt)("inlineCode",{parentName:"p"},"log( sum(\nexp( values )))"),' but this runs the risk of the above numerical problems. Instead, a more numerically stable computation\nuses the "log-sum-exp" formula, which computes the same value but avoids numerical errors by separating out the largest\nvalue. Here is a fairly robust implementation:'),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-R"},"log.sum.exp <- function(\n    x,\n    na.rm = FALSE               # This mimics the na.rm argument of base R's sum() function\n) {\n    result = NA\n    z = max( x )\n    if( !is.na( z )) {\n        terms = x-z\n        if( na.rm ) {\n            w = which( !is.na( terms ) )\n        } else {\n            w = 1:length( terms )\n        }\n        if( length( w ) > 0 ) {\n            result = z + log( sum( exp( terms[w] ) ))\n        }\n    }\n    return( result )\n}\n")),(0,o.kt)("p",null,"Now let's compute the posteriors:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-R"},'for( variant in 1:nrow( sites )) {\n    for( sample in 1:length(samples)) {\n\n        # These three lines implement Bayes rule, but working in log space\n        log.unnormalised.posterior = copy.number.lls[variant,sample,] + log(prior)\n        log.normalisation.constant = log.sum.exp( log.unnormalised.posterior )      # equivalent to: log( sum( exp( log.unnormalised.posterior )))\n        log.posterior = log.unnormalised.posterior - log.normalisation.constant\n        \n        # Now compute expected copy number given the posterior distribution\n        expected.posterior.state[variant,sample] = sum( exp(log.posterior) * copy.numbers )\n    }\n}\n\nplot.copy.numbers( expected.posterior.state, title = "Expected posterior copy number" )\n')),(0,o.kt)("p",null,"Compare this with the maximum likelihood copy number calls above - the posterior version is much much cleaner."),(0,o.kt)("p",null,"It's clear now that there is something goin on in that region - a number of samples have clear runs of deleted or\napparently duplicated bins. To draw out this signal we can cluster samples - here using hierarchical clustering:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},'clustered.order = hclust(\n    # we will cluster based on a distances between expected posterior state vectors\n    dist( t(expected.posterior.state) )             \n)$order\n\nplot.copy.numbers( expected.posterior.state[,clustered.order], title = "Expected posterior copy number (clustered)" )\n')),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Note")," when you get here, please email me (",(0,o.kt)("a",{parentName:"p",href:"mailto:gavin.band@well.ox.ac.uk"},"gavin.band@well.ox.ac.uk"),") with the results of your last plot!"),(0,o.kt)("h2",{id:"future-directions"},"Future directions"),(0,o.kt)("p",null,"This is the end of this part of the tutorial.  However, this model still isn't good enough because it still only works marginally at each site and sample.  In fact, we know how copy number variants generally arise (unequal crossover leading to long runs of duplicated or deleted DNA) and we'd like to include that information in the model too, if only we could figure out how to put it in.  In the ",(0,o.kt)("a",{parentName:"p",href:"/whg-training-resources/statistical_modelling/Hidden_markov_models/glycophorin_cnv_hmm"},"next part of the tutorial")," we will see how that can be done by linking this to a type of model known as a Hidden Markov Model, that models copy number state across the region."))}u.isMDXComponent=!0}}]);